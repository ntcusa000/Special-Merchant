name: Sync images from data/sheet.csv

on:
  workflow_dispatch:
  schedule:
    - cron: "*/10 * * * *" # 建議 10 分鐘；要 5 分鐘就改 */5

permissions:
  contents: write

jobs:
  sync-images:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install requests

      - name: Sync images (download new, remove unreferenced, generate mapping)
        shell: bash
        run: |
          set -euo pipefail

          if [ ! -f data/sheet.csv ]; then
            echo "Missing data/sheet.csv; skip."
            exit 0
          fi

          mkdir -p data/images

          cat > .github_sync_images.py << 'PY'
          import csv, hashlib, json, re, sys, tempfile
          from pathlib import Path
          import requests

          CSV_PATH = Path("data/sheet.csv")
          IMG_DIR = Path("data/images")
          INDEX_JSON = IMG_DIR / "index.json"
          MAPPING_CSV = Path("data/mapping.csv")

          # 一格多張圖片網址：支援逗號/分號/空白/換行分隔
          SPLIT_RE = re.compile(r"[,\n;\s]+")

          # Google Drive share link: https://drive.google.com/file/d/<ID>/view?...
          DRIVE_FILE_RE = re.compile(r"https?://drive\.google\.com/file/d/([a-zA-Z0-9_-]+)")

          def is_url(s: str) -> bool:
              return s.startswith("http://") or s.startswith("https://")

          def sha256_hex(s: str) -> str:
              return hashlib.sha256(s.encode("utf-8")).hexdigest()

          def drive_direct_download(url: str) -> str:
              m = DRIVE_FILE_RE.search(url)
              if not m:
                  return url
              fid = m.group(1)
              return f"https://drive.google.com/uc?export=download&id={fid}"

          def ext_from_content_type(ct: str | None) -> str | None:
              if not ct:
                  return None
              ct = ct.split(";")[0].strip().lower()
              mapping = {
                  "image/jpeg": "jpg",
                  "image/jpg": "jpg",
                  "image/png": "png",
                  "image/webp": "webp",
                  "image/gif": "gif",
                  "image/bmp": "bmp",
                  "image/tiff": "tif",
                  "image/svg+xml": "svg",
              }
              return mapping.get(ct)

          def safe_head_text(path: Path, n: int = 2048) -> str:
              b = path.read_bytes()[:n]
              try:
                  return b.decode("utf-8", errors="ignore").lower()
              except Exception:
                  return ""

          def download_image(original_url: str, tmpdir: Path) -> tuple[str, str, str]:
              """
              return (original_url, final_url, filename)
              filename = sha256(original_url)[:32] + .ext
              """
              final_url = drive_direct_download(original_url)
              h = sha256_hex(original_url)[:32]

              headers = {"User-Agent": "github-actions-image-sync/1.0"}
              resp = requests.get(final_url, headers=headers, timeout=25, stream=True, allow_redirects=True)
              resp.raise_for_status()

              ct = resp.headers.get("Content-Type")
              ext = ext_from_content_type(ct) or "bin"
              filename = f"{h}.{ext}"
              out = tmpdir / filename

              # 防呆：不是 image/* 就拒絕，避免把 HTML 存進 repo
              if ct and not ct.lower().startswith("image/"):
                  # 寫一小段檢查是否 HTML（Drive 常見被導到權限/登入頁）
                  with out.open("wb") as f:
                      for chunk in resp.iter_content(chunk_size=8192):
                          if chunk:
                              f.write(chunk)
                              break
                  head = safe_head_text(out)
                  if "<html" in head or "accounts.google.com" in head or "signin" in head or "permission" in head:
                      raise RuntimeError(f"Blocked/HTML response (content-type={ct}) for {original_url}")
                  raise RuntimeError(f"Non-image content-type={ct} for {original_url}")

              with out.open("wb") as f:
                  for chunk in resp.iter_content(chunk_size=1024*128):
                      if chunk:
                          f.write(chunk)

              if out.stat().st_size == 0:
                  raise RuntimeError(f"Downloaded empty file for {original_url}")

              return original_url, final_url, filename

          def collect_rows_and_urls():
              rows = []
              urls = []

              with CSV_PATH.open("r", encoding="utf-8-sig", newline="") as f:
                  reader = csv.DictReader(f)
                  if not reader.fieldnames:
                      return rows, urls

                  required = ["商家名稱", "優惠名稱", "優惠內容", "地址", "聯絡資訊", "圖片網址"]
                  missing = [c for c in required if c not in reader.fieldnames]
                  if missing:
                      raise RuntimeError(f"CSV missing columns: {missing}. Found: {reader.fieldnames}")

                  for row in reader:
                      cell = (row.get("圖片網址") or "").strip()
                      parts = [p.strip() for p in SPLIT_RE.split(cell) if p.strip()] if cell else []
                      img_urls = [p for p in parts if is_url(p)]
                      row["_img_urls"] = img_urls
                      rows.append(row)
                      urls.extend(img_urls)

              # 去重保序
              seen = set()
              uniq = []
              for u in urls:
                  if u not in seen:
                      seen.add(u)
                      uniq.append(u)
              return rows, uniq

          def write_mapping_csv(mappings: list[dict]):
              with MAPPING_CSV.open("w", encoding="utf-8", newline="") as f:
                  w = csv.DictWriter(f, fieldnames=["商家名稱","優惠名稱","圖片網址","下載網址","檔名"])
                  w.writeheader()
                  for m in mappings:
                      w.writerow(m)

          def main():
              rows, uniq_urls = collect_rows_and_urls()
              print(f"Found {len(uniq_urls)} unique image url(s).")

              IMG_DIR.mkdir(parents=True, exist_ok=True)

              desired_files: set[str] = set()
              url_to_file: dict[str, str] = {}
              url_to_download: dict[str, str] = {}
              mappings: list[dict] = []

              # 全部先下載到 tmp；任一失敗就 abort（避免誤刪+半更新）
              with tempfile.TemporaryDirectory() as td:
                  td_path = Path(td)

                  for u in uniq_urls:
                      try:
                          orig, final, fn = download_image(u, td_path)
                          desired_files.add(fn)
                          url_to_file[orig] = fn
                          url_to_download[orig] = final
                          print(f"OK  {orig} -> {fn}")
                      except Exception as e:
                          print(f"ERR {u} -> {e}")
                          print("Download error occurred; abort without modifying repo files.")
                          return 2

                  # 套用（覆蓋/新增）
                  for fn in desired_files:
                      (IMG_DIR / fn).write_bytes((td_path / fn).read_bytes())

              # 逐列 mapping（保留商家/優惠資訊）
              for r in rows:
                  for u in r.get("_img_urls", []):
                      mappings.append({
                          "商家名稱": (r.get("商家名稱") or "").strip(),
                          "優惠名稱": (r.get("優惠名稱") or "").strip(),
                          "圖片網址": u,
                          "下載網址": url_to_download.get(u, drive_direct_download(u)),
                          "檔名": url_to_file.get(u, ""),
                      })

              # 刪除未被引用的舊圖（保留 index.json）
              keep = desired_files | {INDEX_JSON.name}
              removed = 0
              for p in IMG_DIR.iterdir():
                  if p.is_file() and p.name not in keep:
                      p.unlink()
                      removed += 1

              # index.json（URL -> {file, download}）
              index_obj = {
                  "generated_from": str(CSV_PATH),
                  "count": len(url_to_file),
                  "items": {
                      u: {"file": url_to_file[u], "download": url_to_download.get(u, drive_direct_download(u))}
                      for u in url_to_file
                  }
              }
              INDEX_JSON.write_text(json.dumps(index_obj, ensure_ascii=False, indent=2), encoding="utf-8")

              # mapping.csv（逐商家/逐優惠列）
              write_mapping_csv(mappings)

              print(f"Done. Kept {len(desired_files)} image(s), removed {removed} old file(s).")
              print(f"Generated: {INDEX_JSON} and {MAPPING_CSV}")
              return 0

          if __name__ == "__main__":
              sys.exit(main())
          PY

          python .github_sync_images.py

      - name: Commit if changed (includes new/deleted files)
        shell: bash
        run: |
          set -euo pipefail

          # Stage changes under data/ (mapping + images + index)
          git add -A data/mapping.csv data/images

          if [ -z "$(git status --porcelain)" ]; then
            echo "No changes."
            exit 0
          fi

          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          git commit -m "chore: sync images + mapping from sheet.csv"
          git push
